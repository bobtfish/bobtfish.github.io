<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[t0m's very occasional blog]]></title>
  <link href="http://bobtfish.github.io/atom.xml" rel="self"/>
  <link href="http://bobtfish.github.io/"/>
  <updated>2015-04-12T00:32:17+01:00</updated>
  <id>http://bobtfish.github.io/</id>
  <author>
    <name><![CDATA[Tomas Doran]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DIY Scalable PAAS with Terraform]]></title>
    <link href="http://bobtfish.github.io/blog/2015/04/06/diy-scalable-paas-with-terraform/"/>
    <updated>2015-04-06T23:45:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2015/04/06/diy-scalable-paas-with-terraform</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been ranting about <a href="https://www.terraform.io/">Terraform</a> <a href="http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up/">a</a> <a href="http://bobtfish.github.io/blog/2015/04/03/terraform-0-dot-4-0/">lot</a> recently, and I&rsquo;ve shown some pretty
neat examples of building a VPC &ndash; however before now I haven&rsquo;t got anything <em>actually</em>
useful running.</p>

<p>In this post, we&rsquo;re gonna change that, and launch ourselves a publicly
available and scalable PAAS (Platform As A Service), built around Apache <a href="http://mesos.apache.org/">Mesos</a> and <a href="https://github.com/mesosphere/marathon">Marathon</a>
with <a href="http://nginx.org/">nginx</a> and Amazon&rsquo;s <a href="http://aws.amazon.com/elasticloadbalancing/">ELB</a> for load balancing and <a href="http://aws.amazon.com/route53/">Route53</a> for DNS.</p>

<!-- more -->


<h2>Getting setup</h2>

<p>You&rsquo;ll need an AWS account, a Ubuntu machine and a domain you control (which doesn&rsquo;t have to be in AWS) for this demo.
My example domain is <em>notanisp.net</em>, and so the subdomain I&rsquo;m going to use in all the examples is <em>mesos.notanisp.net</em></p>

<p>You&rsquo;ll also need the <a href="http://aws.amazon.com/cli/">AWS cli</a> and an <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-config-files">~/.aws/credentials</a>
file with a <em>[demo]</em> section in it.</p>

<p>In your AWS account, you need to go into the IAM menus, and create a &lsquo;<a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/roles-usingrole-instanceprofile.html">Role</a>&rsquo;,
which will be given to the machines we launch, to <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ExamplePolicies_EC2.html">allow them to find out other instances</a>.</p>

<p>Your policy should be named <em>describe-instances</em> and the body of the policy should contain an inline policy:</p>

<pre><code>{
   "Version": "2012-10-17",
   "Statement": [{
      "Effect": "Allow",
      "Action": "ec2:Describe*",
      "Resource": "*"
    }
   ]
}
</code></pre>

<p>If you&rsquo;re feeling lazy, then you can just create the role with the policy of AmazonEC2ReadOnlyAccess.</p>

<h2>Getting terraform</h2>

<p>The easiest way to get my patched version of terraform is to build it in vagrant:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/bobtfish/terraform.git
</span><span class='line'>cd terraform
</span><span class='line'>vagrant up
</span><span class='line'>vagrant ssh
</span><span class='line'>sudo apt-get install -y ruby ruby-json python-pip
</span><span class='line'>sudo pip install awscli
</span><span class='line'>export PATH=$PATH:/opt/go/bin:/opt/gopath/bin
</span><span class='line'>echo 'export PATH=$PATH:/opt/go/bin:/opt/gopath/bin' &gt;&gt; ~/.bashrc
</span><span class='line'>mkdir -p /opt/gopath/src/github.com/hashicorp
</span><span class='line'>cp -r /vagrant /opt/gopath/src/github.com/hashicorp/terraform
</span><span class='line'>cd /opt/gopath/src/github.com/hashicorp/terraform
</span><span class='line'>make updatedeps
</span><span class='line'>make dev
</span><span class='line'>cd ~/
</span><span class='line'>git clone https://github.com/bobtfish/terraform-example-mesos-cluster
</span><span class='line'>mkdir ~/.aws
</span><span class='line'>vi ~/.aws/credentials</span></code></pre></td></tr></table></div></figure>


<p>Adding something like this:</p>

<pre><code>[demo]
aws_access_key_id=AAAAAAAA
aws_secret_access_key=BBBBBBBBBBBBBBBBB
</code></pre>

<p>Or if you prefer (and you trust me!), <a href="http://notanisp-packages.s3-website-eu-west-1.amazonaws.com/dists/precise/main/binary-amd64/terraform_0.4.0-bobtfish0_amd64.deb">here&rsquo;s a .deb</a>
you should be able to just install:</p>

<h2>Building your cluster</h2>

<p>Once you&rsquo;re all setup, you can get started building your cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd terraform-example-mesos-cluster
</span><span class='line'>make</span></code></pre></td></tr></table></div></figure>


<p>This generates an ssh key (stored in <em>id_rsa</em>) to be able to ssh into your instances,
and captures your IP (stored in <em>admin_iprange.txt</em>) to allow access to the Mesos and
Marathon admin interfaces later. Note: You can edit the admin_iprange.txt file to
allow more IPs access, but it is <em>not</em> recommended to change this to 0.0.0.0/0,
as otherwise random people on the internet will be able to run jobs on your Mesos
cluster!</p>

<p>Lets go ahead and build out the cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd eu-central-1
</span><span class='line'>vi terraform.tfvars # Adjust the 'domain' value to a subdomain
</span><span class='line'>make # This pulls down all the terraform modules you need, and builds a map of which AZs your account can see.
</span><span class='line'>terraform apply</span></code></pre></td></tr></table></div></figure>


<p>The last step will take a while, go grab a coffee.</p>

<p>Eventually, terraform will finish, with output like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Apply complete! Resources: 35 added, 0 changed, 0 destroyed.
</span><span class='line'>
</span><span class='line'>The state of your infrastructure has been saved to the path
</span><span class='line'>below. This state is required to modify and destroy your
</span><span class='line'>infrastructure, so keep it safe. To inspect the complete state
</span><span class='line'>use the `terraform show` command.
</span><span class='line'>
</span><span class='line'>State path: terraform.tfstate
</span><span class='line'>
</span><span class='line'>Outputs:
</span><span class='line'>
</span><span class='line'>  marathon_api  = http://marathon.admin.mesos.notanisp.net
</span><span class='line'>  nat_public_ip = 52.28.37.228</span></code></pre></td></tr></table></div></figure>


<p>Once it&rsquo;s finished, go into the route 53 control panel, and you should see your subdomain zone.
Go and grab its NS records:</p>

<p><img src="https://raw.githubusercontent.com/bobtfish/terraform-example-mesos-cluster/master/route53.png" alt="Route53 console" /></p>

<p>and put those into whatever you use to manage your main domain as a delegated record.</p>

<h2>Gettins admin pages and launching an app</h2>

<p>After DNS catches up, you should be able to resolve:</p>

<ul>
<li>mesos.admin.mesos.notanisp.net</li>
<li>marathon.admin.mesos.notanisp.net</li>
<li>www.mesos.notanisp.net</li>
</ul>


<p>Hit <a href="http://marathon.admin.mesos.notanisp.net">http://marathon.admin.mesos.notanisp.net</a> with your browser, and you should be able to see the
Marathon admin screen. (And you can see the Mesos admin screen at <a href="http://mesos.admin.mesos.notanisp.net">http://mesos.admin.mesos.notanisp.net</a>)</p>

<p>You can now <a href="https://mesosphere.com/docs/tutorials/run-services-with-marathon">launch an app</a>!</p>

<p>I&rsquo;ve automated the launching of an example application, so you can just say:</p>

<pre><code>make deploywww
</code></pre>

<p>This will deploy <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/marathon_www.json">the hello world app</a>
 (from the mesosphere tutorial linked above) named &lsquo;/www&rsquo;
into marathon:</p>

<p><img src="http://bobtfish.github.io/images/marathon_deploy.png" alt="App deploying" /></p>

<p>and after a couple of mins, you should be able to access it from <a href="http://www.mesos.notanisp.net">http://www.mesos.notanisp.net</a></p>

<p>Any additional apps you launch in Marathon with names like /someapp will be automatically bound
to a vhost on the load balancer. Pretty neat, eh?</p>

<h2>What we&rsquo;re building behind the scenes.</h2>

<p>We <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/vpc.tf">ask for a VPC to be built</a>
(with the <a href="https://github.com/bobtfish/terraform-vpc-nat">module I created</a> in <a href="http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up/">a previous post</a>).</p>

<p>This gives us &lsquo;private&rsquo; and &lsquo;public&rsquo; subnets, and a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html">NAT instance</a>.</p>

<p>On top of that, we build out a <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/mesos.tf">mesos cluster</a></p>

<p>This comprises a number of pieces, first of all <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/mesos_master/master.conf">masters</a>, which
run <a href="https://zookeeper.apache.org/">zookeeper</a> in addition to Marathon and Mesos. We run 3 of these by default, so one can fail without
affecting the operation of the cluster. Also there are <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/mesos_slave/slave.conf">slaves</a>
which actually run the Mesos workers and excute applications. We launch 3 of these, and if one fails then any tasks it was running
will be re-launched on the remaining slaves (assuming they have enough resources to do so!).</p>

<p>Then we deploy 2 <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/lb/lb.conf">load balancers</a> which discover the running applications from the marathon
API and configure nginx, with a publicly accessible <a href="http://aws.amazon.com/elasticloadbalancing/">ELB</a> <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/elb/main.tf">in front of them</a>
 to provide HA, in the event one of the load balancer machines fails.</p>

<p>We push this <a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/dns/main.tf">into DNS</a>, along with records for the <em>adminlb</em> machine.
<a href="https://github.com/bobtfish/tf_aws_mesos/blob/master/elb/main.tf">This machine</a> is what proxies the mesos and marathon admin pages (and access to it
is locked down to your <em>admin_iprange.txt</em>)</p>

<h2>Redundancy and scaling</h2>

<p>The cluster should be redundant to a load balancer or a mesos master failing. If the instance which fails can&rsquo;t be brought back
online, it should be possible to just terminate the instance and have terraform re-create it.</p>

<p>Also, you can eaisly adjust the number of slaves dynamically (just by <a href="https://github.com/bobtfish/terraform-example-mesos-cluster/blob/master/eucentral1-demo/mesos.tf#L6">changing the variable</a>
 and running terraform), or the instance types employed (by rebuilding the cluster) if you&rsquo;d like to run more tasks, serve real
load from the cluster, or scale whilst already serving real load!</p>

<p>Marathon apps which have already been deployed can be scaled up (to more instances) just by using the Marathon API with a web browser &ndash; I encourage you to try this, scaling
the example app up to 5 instances, or down to 1 instance and then killing 4/5 of the slaves.</p>

<h2>TODOs</h2>

<ul>
<li><p>Currently the machine&rsquo;s Internet access goes through a single NAT instance with no failover &ndash; we now deploy a NAT instance per AZ, so solutions are either:</p>

<ul>
<li>Have a route table per AZ to send traffic for each AZ out that AZ&rsquo;s NAT box</li>
<li>Have failover between NAT boxes (keepalived+modifying the route table(s))</li>
</ul>
</li>
<li><p>The machines have no configuration management (no puppet/chef), which means that making any changes to them (or getting
any security updates) involves rebuilding the instances.</p></li>
</ul>


<h2>Conclusion</h2>

<p>Hopefully this post has shown that Terraform is already grown up enough to be used to compose real, production ready infrastructures.</p>

<p>The example shown here <em>is not</em> something I&rsquo;d run as a production system, and there are a number of significant warts
in the code (like having to pack and unpack lists into comma seperated strings), but there&rsquo;s enough of the essential complexity
represented here to be close to &lsquo;real&rsquo; applications and deployment, and I&rsquo;m still having a great time exploring Terraform&rsquo;s
rapidly growing capabilities, and producing reuseable modules.</p>

<p>Whilst I don&rsquo;t think that anything but my most basic level of modules (e.g. AMI lookups) are likely to be reused verbatim,
I hope this example shows that it&rsquo;s possible to use terraform as a turnkey component in building repeatable
and composable infrastructures.</p>

<h2>Credits</h2>

<p>Container Solutions have <a href="http://container-solutions.com/2015/04/how-to-set-up-mesos-on-google-cloud-with-terraform/">an excellent blog post</a>
on making a Mesos cluster on <a href="https://cloud.google.com/compute/">GCE</a> &ndash; I stole it, converted it to AWS and added the service discovery/load balancing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terraform 0.4.0]]></title>
    <link href="http://bobtfish.github.io/blog/2015/04/03/terraform-0-dot-4-0/"/>
    <updated>2015-04-03T00:48:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2015/04/03/terraform-0-dot-4-0</id>
    <content type="html"><![CDATA[<p><a href="https://hashicorp.com/blog/terraform-0-4.html">Terraform 0.4.0 is out</a>, with the <a href="https://github.com/hashicorp/terraform/pull/1185">remote modules</a> feature!
As per <a href="http://localhost:4000/blog/2015/03/29/terraform-from-the-ground-up/">my last post</a>,
I&rsquo;ve been playing around with this already, and I thought it was worth showing
off my simple example of this feature actually being used.</p>

<p>It&rsquo;s wrapped up in a way that you can fork my 2 repositories and play with it
yourself, and I encourage <a href="https://github.com/bobtfish/terraform-example-vpc-infra/blob/master/eucentral1-demo/terraform.tfvars#L3">you to do so</a></p>

<!-- more -->


<p>If you take <a href="https://github.com/bobtfish/terraform-example-vpc/tree/master/eucentral1-demo">the example VPC</a>
from my last post (with the state file committed), you can now pull in the data in another terraform
repository <a href="https://github.com/bobtfish/terraform-example-vpc-infra/blob/master/eucentral1-demo/vpc.tf">like this</a>
and then use the outputs it defines <a href="https://github.com/bobtfish/terraform-example-vpc-infra/blob/master/eucentral1-demo/mesos.tf#L8">like this</a>.</p>

<p>This is pretty neat, and it&rsquo;ll be super cool to allow multiple teams to work on different layers
of the infrastructure, using the <a href="https://www.terraform.io/docs/commands/remote.html">consul state store</a> and <a href="https://www.consul.io/docs/internals/acl.html">ACLs</a>,
more news on that once I&rsquo;ve had chance to play with it.</p>

<p>Unfortunately, the credentials file feature didn&rsquo;t make 0.4.0, and I <a href="https://github.com/bobtfish/terraform-aws-coreos-kubernates-cluster/blob/master/nodes.tf#L28">do math that isn&rsquo;t possible in master</a> now, so you&rsquo;ll still need to use my fork if you want to use my examples verbatim.</p>

<p>I&rsquo;ve written a bunch of modules to power my examples though, most of which work on unpatched terraform, and I thought that I&rsquo;d
list them (in most to least reuseable order) so that you don&rsquo;t have to dig through all the code or my github to find them :)</p>

<ul>
<li><a href="https://github.com/terraform-community-modules/tf_aws_virttype">tf_aws_virttype</a> &ndash; Match an AMI type (e.g. m3.xlarge) to its virtualization type (e.g. hvm)</li>
<li><a href="https://github.com/terraform-community-modules/tf_aws_availability_zones">tf_aws_availability_zones</a> &ndash; Work out which azs each of your accounts has access to for easy lookup/templating across accounts.</li>
<li><a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami">tf_aws_ubuntu_ami</a> &ndash; Look up Ubuntu <a href="http://cloud-images.ubuntu.com/locator/ec2/">AMIs</a></li>
<li><a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami">tf_aws_coreos_ami</a> &ndash; Look up the most recent <a href="https://coreos.com/">CoreOS</a> <a href="https://coreos.com/docs/running-coreos/cloud-providers/ec2/">AMI</a></li>
<li><a href="https://github.com/bobtfish/terraform-vpc">terraform-vpc</a> &ndash; Build out an initial VPC with 2 AZs</li>
<li><a href="https://github.com/bobtfish/terraform-vpc-nat">terraform-vpc-nat</a> &ndash; Build NAT instances out on top of a VPC made with terraform-vpc</li>
<li><a href="https://github.com/bobtfish/tf_aws_mesos">tf_aws_mesos</a> &ndash; Launch a Mesos cluster into a VPC and across multiple AZs</li>
<li><a href="https://github.com/bobtfish/terraform-aws-coreos-kubernates-cluster">terraform-aws-coreos-kubernates-cluster</a> &ndash; Kubernates cluster on CoreOS (n.b. not yet working out the box, needs patched terraform)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terraform from the ground up]]></title>
    <link href="http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up/"/>
    <updated>2015-03-29T21:18:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2015/03/29/terraform-from-the-ground-up</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been playing around with <a href="https://www.terraform.io/">Terraform</a> a bunch recently, and I&rsquo;m pretty <a href="https://twitter.com/bobtfish/status/581905067948797952">excited</a> about <a href="https://github.com/hashicorp/terraform/blob/master/CHANGELOG.md#040-unreleased">0.4.0</a>.</p>

<p>However at the moment, the examples leave quite a lot to be desired. So for my own learning and entertainment, I&rsquo;ve created a set of example Terraform modules,
and a simple example that you should be able to clone and run.</p>

<!-- more -->


<h2>What&rsquo;s exciting in Terraform</h2>

<p>One of the <a href="https://github.com/hashicorp/terraform/pull/1319">pull requests</a> <a href="https://github.com/hashicorp/terraform/pull/1080">I&rsquo;ve been watching</a> <a href="around%20ASGs">https://github.com/hashicorp/terraform/pull/1076</a> / etc were merged recently, <a href="https://github.com/hashicorp/terraform/issues/932">fixing Tags on ASGs</a> which I use at work for the <a href="https://docs.puppetlabs.com/guides/external_nodes.html">puppet ENC</a>, and the <a href="https://github.com/hashicorp/terraform/pull/1049">support for ~/.aws/credentials</a> files is almost ready.</p>

<p>In fact, after chatting with folks at <a href="http://www.scalesummit.org/">scalesummit</a> on Friday, I was so excited that I wanted to pull some of the new features into
<a href="https://github.com/bobtfish/terraform">my fork</a> and play with them.</p>

<p>One of the most exciting strategic features to me is what&rsquo;s been called &lsquo;<a href="https://github.com/hashicorp/terraform/pull/1185">remote modules</a>&rsquo;,
which when it&rsquo;s merged will allow you to consume external Terraform state (in a read-only way) from other Terraform repositories.</p>

<p>This is <em>almost exactly</em> one of the features I&rsquo;d sketched out as something Terraform needed to allow it to scale as a tool for larger organisations.</p>

<p>At my day job, we have multiple teams managing different parts of the infrastructure &ndash; for example one team is responsible for VPCs
and DNS/puppet masters etc, whilst another team is responsible for kafka/zookeeper clusters, and several other teams all having
independently managed Elasticsearch clusters.</p>

<p>Ergo the ability to &lsquo;publish&rsquo; state between teams and thus allowing different teams to share the basics like VPCs and subnets)
whilst having their own configs is essential for the variety of workflows and machine lifecycles that I need to support.</p>

<p>I haven&rsquo;t yet tested out this functionality, as I <a href="http://www.globalnerdy.com/wordpress/wp-content/uploads/2012/09/yak-shaving.jpg">got distracted</a>
making some example &lsquo;base infrastructure&rsquo; modules that I wanted to consume data from.</p>

<h2>Unreleased software note!</h2>

<p>You need to use <a href="https://github.com/bobtfish/terraform">my fork</a> of Terraform for the examples below to work,
but I expect them to work without any significant changes in 0.4.0</p>

<h2>Terraform modules</h2>

<p>I&rsquo;ve scratched my head about how to do Terraform modules (which need to lookup external data) for a while, and I&rsquo;ve come up with a pattern that I don&rsquo;t hate.</p>

<p>I&rsquo;ve also written some public modules that are on github, which you can look at and criticise. (Sorry in advance for the terrible ruby)</p>

<p>Whilst I haven&rsquo;t yet used the feature I merged, the state from the example in this post is a semi-realistic example VPC
that I&rsquo;ll be able to use for further testing.</p>

<p>To give you some insight into how I&rsquo;ve put things together, I&rsquo;m going to dive into
each of the modules and explain a little about them, before
showing how I wrap them up together with the actual code/config to launch a VPC.</p>

<h3>Looking up AMIs</h3>

<p>Lets think about how we should lookup a Ubuntu AMI using <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami">a module</a>.</p>

<p>There&rsquo;s <a href="http://cloud-images.ubuntu.com/locator/ec2">a giant table</a> of AMIs available on the web, and the data for it is <a href="http://cloud-images.ubuntu.com/locator/ec2/releasesTable">almost</a>, <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/getvariables.rb#L11">but not quite</a> JSON you can parse.</p>

<p>So I wrote a getvariables.rb script, a <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/Makefile">Makefile</a> which generates <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/variables.tf.json">variables.tf.json</a> &ndash; end result, we have a giant hash table of all the AMIs for Ubuntu in all of the regions.</p>

<p>We can then use a combination of variables and the <a href="https://github.com/bobtfish/terraform/blob/master/website/source/docs/configuration/interpolation.html.md#user-content-built-in-functions">lookup + format functions</a> in <a href="https://github.com/terraform-community-modules/tf_aws_ubuntu_ami/blob/master/main.tf">the main.tf</a> to output the desired AMI.</p>

<p>Then we just use the module, supplying it the params it needs, and we get the right AMI.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>module "ami" {
</span><span class='line'>  source = "github.com/terraform-community-modules/tf_aws_ubuntu_ami"
</span><span class='line'>  region = "eu-central-1"
</span><span class='line'>  distribution = "trusty"
</span><span class='line'>  architecture = "amd64"
</span><span class='line'>  virttype = "hvm"
</span><span class='line'>  storagetype = "instance-store"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>resource "aws_instance" "web" {
</span><span class='line'>  ami = "${module.ami.ami_id}"
</span><span class='line'>  instance_type = "m3.8xlarge"
</span><span class='line'>  ...
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h3>What availability zones?</h3>

<p>Next example &ndash; in AWS, which availability zones you&rsquo;re given access to depends on your account.</p>

<p>Therefore, to provide a generic template that anyone can use to launch a VPC (in any region), we need to be able
to detect which regions and availability zones a user has access to.</p>

<p>Using almost the same pattern, and <a href="http://aws.amazon.com/cli/">the aws cli tool</a>, I&rsquo;ve produced another module
which will read your <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-config-files">~/.aws/credentials file</a> to
pull in a list of all your available availability zones.</p>

<p>Due to the way Terraform currently handles things, <a href="https://github.com/bobtfish/terraform-azs">this module</a> just exports three variables, a primary, secondary, and (where available)
tertiary availability zone (ordered by alphabetical sort).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>module "az" {
</span><span class='line'>  source = "github.com/terraform-community-modules/tf_aws_availability_zones"
</span><span class='line'>  region = "eu-central-1"
</span><span class='line'>  account = "demo"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>resource "aws_subnet" "primary-front" {
</span><span class='line'>  availability_zone = "${module.az.primary}"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Note that for this to work, you will need a <em>[demo]</em> section in your ~/.aws/credentials file, as the variables.tf.json file
is <em>not</em> committed to the repository, unlike the last example.</p>

<h3>Lets build some damn infrastructure already!</h3>

<p>We&rsquo;ve now got the pieces we need to put together a VPC, so lets go ahead and <a href="https://github.com/bobtfish/terraform-vpc">write a module to do that</a>.</p>

<p>This takes a /16 network and builds out a VPC with public (internet IP), private (fixed address) and ephemeral (ASG/ELB) subnets
in two availability zones.</p>

<p>Note that we have <em>a lot</em> <a href="https://github.com/bobtfish/terraform-vpc/blob/master/outputs.tf">of outputs here</a>, as we need to expose
every piece of infastructure we want to be able to reference to the module&rsquo;s users.</p>

<h3>But I meant an actual machine!</h3>

<p>That&rsquo;s the next step, we want to launch <a href="https://github.com/bobtfish/terraform-vpc-nat/blob/master/main.tf#L47">our first NAT machine</a>, and again, we&rsquo;ll <a href="https://github.com/bobtfish/terraform-vpc-nat">write a module for it</a>!</p>

<p>We use <a href="https://help.ubuntu.com/community/CloudInit#Cloud_Config_Syntax">cloud-config</a> to setup a firewall on the initial machine, so that subsequent
machines (without public IP addresses) can NAT externally, and we reset the main routing table (used by the &lsquo;back&rsquo; and &lsquo;ephemeral&rsquo; subnets created
by the VPC module) to be one which points to the new NAT instance. For <a href="https://pbs.twimg.com/media/CBHd3OVUwAA8FBh.png:large">good measure</a> we also install <a href="https://www.docker.com/">Docker</a> and <a href="https://puppetlabs.com/">puppet</a>.</p>

<p>Note that we have to use the remote_exec provisioner here to wait for cloud-init to finish, so that we know the firewall rules are in place
for NAT before we launch any more machines</p>

<h2>So when do we get to do something I can actually run?</h2>

<p>Ok, lets put the VPC + nat machine module to work, and <a href="https://github.com/bobtfish/terraform-example-vpc/blob/master/eucentral1-demo/internal.tf">build a host inside the private
subnets</a>.</p>

<p>You can (and should!) fork and clone <a href="https://github.com/bobtfish/terraform-example-vpc">this example</a>, which contains
just enough stuff to string the modules we&rsquo;ve written together and bring up a couple of hosts.</p>

<p>You&rsquo;ll need:</p>

<ul>
<li><a href="https://github.com/bobtfish/terraform">my fork</a> of Terraform</li>
<li><a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-config-files">An ~/.aws/credentials file</a> with a <em>[demo]</em> section</li>
</ul>


<p>Lets go:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>~ ‚≠ê git clone git@github.com:bobtfish/terraform-example-vpc.git
</span><span class='line'>Cloning into 'terraform-example-vpc'...
</span><span class='line'>remote: Counting objects: 61, done.
</span><span class='line'>remote: Compressing objects: 100% (34/34), done.
</span><span class='line'>remote: Total 61 (delta 27), reused 54 (delta 20), pack-reused 0
</span><span class='line'>Receiving objects: 100% (61/61), 15.01 KiB | 0 bytes/s, done.
</span><span class='line'>Resolving deltas: 100% (27/27), done.
</span><span class='line'>Checking connectivity... done.
</span><span class='line'>~ ‚≠ê cd terraform-example-vpc/
</span><span class='line'>terraform-example-vpc (masterüëÜ ‚úî) ‚≠ê make
</span><span class='line'>ssh-keygen -t rsa -f id_rsa -N ''
</span><span class='line'>Generating public/private rsa key pair.
</span><span class='line'>Your identification has been saved in id_rsa.
</span><span class='line'>Your public key has been saved in id_rsa.pub.
</span><span class='line'>The key fingerprint is:3d:c8:0c:76:1e:29:25:a2:52:cd:7d:09:9b:53:24:c9 t0m@somebox
</span><span class='line'>The key's randomart image is:
</span><span class='line'>+--[ RSA 2048]----+
</span><span class='line'>|  .+.o+o+.       |
</span><span class='line'>| . .E..Bo.       |
</span><span class='line'>|. .   B.+        |
</span><span class='line'>| .   . O +       |
</span><span class='line'>|        S o      |
</span><span class='line'>|           o     |
</span><span class='line'>|                 |
</span><span class='line'>|                 |
</span><span class='line'>|                 |
</span><span class='line'>+-----------------+
</span><span class='line'>true
</span><span class='line'>terraform-example-vpc (masterüëÜ ‚úî) ‚≠ê</span></code></pre></td></tr></table></div></figure>


<p>This has setup an ssh key to allow you to log into the created instances.</p>

<p>Now, change directory into the region+account folder (<a href="https://github.com/hashicorp/terraform/pull/1281">Terraform can only do one region at once currently</a>),
and run make again. This pulls in all the necessary modules (using terraform get), and then runs their Makefiles by iterating over .terraform/modules to ensure that <em>variables.tf.json</em> is built it needed.</p>

<p>Note that even modules which don&rsquo;t need to build <em>variables.tf.json</em> are required to have a Makefile (which can do nothing).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>eucentral1-demo (masterüëÜ ‚úî) ‚≠ê make
</span><span class='line'>terraform get
</span><span class='line'>Get: git::https://github.com/terraform-community-modules/tf_aws_ubuntu_ami.git
</span><span class='line'>Get: git::https://github.com/bobtfish/terraform-vpc-nat.git
</span><span class='line'>Get: git::https://github.com/bobtfish/terraform-vpc.git
</span><span class='line'>Get: git::https://github.com/terraform-community-modules/tf_aws_ubuntu_ami.git
</span><span class='line'>Get: git::https://github.com/bobtfish/terraform-azs.git
</span><span class='line'>for i in $(ls .terraform/modules/); do make -C ".terraform/modules/$i"; done
</span><span class='line'>make[1]: Entering directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/1b2917ba643e4bf90b900b6da59b2e05'
</span><span class='line'>ruby getvariables.rb
</span><span class='line'>make[1]: Leaving directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/1b2917ba643e4bf90b900b6da59b2e05'
</span><span class='line'>make[1]: Entering directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/3f6892e390615d3c9f3cc69d65d7291c'
</span><span class='line'>true
</span><span class='line'>make[1]: Leaving directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/3f6892e390615d3c9f3cc69d65d7291c'
</span><span class='line'>make[1]: Entering directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/be339b8825e57c8cb0372c530eaf49bf'
</span><span class='line'>true
</span><span class='line'>make[1]: Leaving directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/be339b8825e57c8cb0372c530eaf49bf'
</span><span class='line'>make[1]: Entering directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/d4386a9079b869044647990c72b8f0e4'
</span><span class='line'>make[1]: Nothing to be done for `all'.
</span><span class='line'>make[1]: Leaving directory `/home/t0m/terraform-example-vpc/eucentral1-demo/.terraform/modules/d4386a9079b869044647990c72b8f0e4'
</span><span class='line'>ruby getvariables.rb &gt; variables.tf.json
</span><span class='line'>eucentral1-demo (masterüëÜ ‚úî) ‚≠ê terraform apply
</span><span class='line'>aws_key_pair.deployer: Creating...
</span><span class='line'>
</span><span class='line'>... loads more stuff deleted ...
</span><span class='line'>
</span><span class='line'>Outputs:
</span><span class='line'>
</span><span class='line'>  account                   = demo
</span><span class='line'>  admin_key_name            = deployer-key
</span><span class='line'>  azs                       = eu-central-1a,eu-central-1b
</span><span class='line'>  cidr_block                = 10.1.0.0/16
</span><span class='line'>  dedicatedsubnets          = subnet-8a1ec9e3,subnet-0ea25475
</span><span class='line'>  default_network_acl_id    = acl-d14488b8
</span><span class='line'>  default_security_group_id = sg-16cc1f7f
</span><span class='line'>  ephemeralsubnets          = subnet-8d1ec9e4,subnet-3fa05644
</span><span class='line'>  frontsubnets              = subnet-8c1ec9e5,subnet-0da25476
</span><span class='line'>  id                        = vpc-540dcf3d
</span><span class='line'>  main_route_table_id       = rtb-998d42f0
</span><span class='line'>  nat_instances             = i-91782a5f,i-e7d96d26
</span><span class='line'>  nat_private_ips           = 10.1.0.99,10.1.1.75
</span><span class='line'>  nat_public_ips            = 52.28.54.125,52.28.54.128
</span><span class='line'>  networkprefix             = 10.1
</span><span class='line'>  private-routetable        = rtb-998d42f0
</span><span class='line'>  public-routetable         = rtb-958e41fc
</span><span class='line'>  region                    = eu-central-1
</span><span class='line'>  security_group_allow_all  = sg-edcc1f84</span></code></pre></td></tr></table></div></figure>


<p>You should be able to ssh to your nat instance:</p>

<pre><code>ssh-add ../id_rsa
make sshnat
</code></pre>

<p>and from there, ssh into your back network host:</p>

<pre><code>ssh 10.1.10.4
</code></pre>

<p>and you should see <a href="https://www.consul.io/">consul</a> running (in Docker <a href="https://pbs.twimg.com/media/CBHd3OVUwAA8FBh.png:large">of course</a>)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ubuntu@ip-10-1-1-4:~$ ping -c 2 8.8.8.8
</span><span class='line'>PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
</span><span class='line'>64 bytes from 8.8.8.8: icmp_seq=1 ttl=56 time=6.74 ms
</span><span class='line'>64 bytes from 8.8.8.8: icmp_seq=2 ttl=56 time=6.96 ms
</span><span class='line'>
</span><span class='line'>--- 8.8.8.8 ping statistics ---
</span><span class='line'>2 packets transmitted, 2 received, 0% packet loss, time 1001ms
</span><span class='line'>rtt min/avg/max/mdev = 6.746/6.856/6.966/0.110 ms
</span><span class='line'>ubuntu@ip-10-1-1-4:~$ sudo docker ps
</span><span class='line'>CONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS              PORTS                                                      NAMES
</span><span class='line'>d63b2034bf80        fhalim/consul:latest   "/bin/sh -c '/usr/lo   6 seconds ago       Up 6 seconds        8400/tcp, 0.0.0.0:8500-&gt;8500/tcp, 0.0.0.0:8600-&gt;8600/udp   consul</span></code></pre></td></tr></table></div></figure>


<h2>Reusing</h2>

<p>As most of the logic is in modules, to create another VPC in another environment or account, the top level directory can just be copied,
the terraform.tfstate file removed and the details in <em>terraform.tfvars</em> updated.</p>

<h2>Conclusion</h2>

<p>Whilst a bunch of stuff in the current demo is more simplistic than a real infrastructure, it shows that Terraform can be used to
bootstrap entire VPCs with non-trivial configurations.</p>

<p>I haven&rsquo;t (yet) installed anything &lsquo;real&rsquo; other than a docker container and some iptables rules as cloud-init is a terrible
config management mechanism! However the current example should be useful as a platform for jumping off from, either
on top of pre-existing infrastructre and puppet masters/chef servers (add vpc peering or vpns), or with ssh based config management
(ansible, salt).</p>

<h2>What&rsquo;s next?</h2>

<p>In the next post (or rather, in my next set of hacking that I may or may not write up ;), I plan to use the infrastructure details of this subnet to test out
the remote module feaure, try to get a puppetmaster (with EC2 tags as the ENC) up, and explore storing the state in <a href="https://www.consul.io/">consul</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[More talks]]></title>
    <link href="http://bobtfish.github.io/blog/2014/11/10/more-talks/"/>
    <updated>2014-11-10T14:01:00+00:00</updated>
    <id>http://bobtfish.github.io/blog/2014/11/10/more-talks</id>
    <content type="html"><![CDATA[<p>I&rsquo;m still rubbish at remembeing to (or finding the time to) blog, however here&rsquo;s some more video links:</p>

<ul>
<li> <a href="http://www.meetup.com/London-DevOps/events/202106482/">London Devops</a> Dockersh and a guided tour of the Linux kernel facilities docker uses. <a href="http://www.youtube.com/watch?v=hwWvfAGkcmg">Video</a> <a href="http://www.slideshare.net/bobtfish/dockersh-and-a-brief-intro-to-the-docker-internals">Slides</a></li>
<li> <a href="http://2014.puppetconf.com/">Puppetconf 2014</a> Sensu and Sensibility &ndash; The Story of a Journey From #monitoringsucks to #monitoringlove. <a href="http://www.youtube.com/watch?v=0VfSmITEOHM">Video</a> <a href="http://www.slideshare.net/bobtfish/sensu-and-sensibility-puppetconf-2014">Slides</a></li>
</ul>


<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Me, in Video form]]></title>
    <link href="http://bobtfish.github.io/blog/2014/04/15/me-in_video_form/"/>
    <updated>2014-04-15T13:38:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2014/04/15/me-in_video_form</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been a massive fail at blogging recently, as I&rsquo;ve been way too busy doing cool stuff :)</p>

<p>However, I have talked at a whole bunch of conferences and tech meetups in the last 12 months,
here&rsquo;s a quick run-down of where&rsquo;s I&rsquo;ve been for posterity:</p>

<h2>Updates since the original post</h2>

<ul>
<li><a href="http://puppetlabs.com/events/puppet-camp-nyc">Puppetcamp NY</a> Streamlining your puppet development workflow. <a href="http://www.youtube.com/watch?v=lRmAMqrPd4k">Video</a> <a href="http://www.slideshare.net/bobtfish/steamlining-your-puppet-developmentpuppetconfny2014">Slides</a></li>
<li><a href="http://www.dockercon.com/">Dockercon 14</a> &ndash; Building a smarter application stack &ndash; service discovery and wiring for Docker and traditional deployments. <a href="https://www.youtube.com/watch?v=49_5lwGtkmo">Video</a> <a href="http://www.slideshare.net/bobtfish/docker-confjune2014">Slides</a></li>
</ul>


<h2>Original list</h2>

<ul>
<li><a href="https://puppetlabs.com/events/puppet-camp-london-0">Puppet camp London 2014</a> &ndash; Chasing AMI &ndash; Building Amazon machine images with Puppet, Packer and Jenkins <a href="http://www.youtube.com/watch?v=ZeCoHYwfn3g">Video</a> <a href="http://www.slideshare.net/bobtfish/chasing-ami-puppetcamplondonapr2014">Slides</a></li>
<li>San Francisco Perl Mongers Tech Meetup May 2014 &ndash; Taking control of chaos with Docker (redux, with additional content) <a href="https://archive.org/details/tomasdorantakingcontrolofchaoswithdocker">Video</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-camp-silicon-valley">Puppet camp Silicon Valley 2014</a> &ndash; Deploying puppet code at light speed <a href="http://www.slideshare.net/bobtfish/deploying-puppet-code-at-light-speed">Slides</a> <a href="http://www.youtube.com/watch?v=CKdNDEyq1eA">Video</a> <a href="http://www.youtube.com/watch?v=W4Skk5D6drU">Panel session video</a></li>
<li><a href="http://www.netways.de/puppetcamp/archive_munich_2013/">Puppet camp Munich 2013</a> &ndash; Thinking through puppet code layout <a href="http://www.slideshare.net/bobtfish/code-layout-puppetcampmunich">Slides</a> <a href="http://www.youtube.com/watch?v=aiJ1pmrqR7E">Video</a></li>
<li>Puppet camp London 2013 &ndash; Taking control of chaos with Docker and Puppet <a href="http://www.slideshare.net/bobtfish/docker-puppetcamp-london-2013">Slides</a> <a href="http://www.youtube.com/watch?v=KSo_mcJxFIA">Video</a></li>
<li>Puppet conf 2013 &ndash; Test driven infrastructure development <a href="">Slides</a> <a href="http://www.youtube.com/watch?v=S8_a5G1UCdM">Video</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[We have always been at war with eastasia]]></title>
    <link href="http://bobtfish.github.io/blog/2013/11/06/we-have-always-been-at-war-with-eastasia/"/>
    <updated>2013-11-06T23:12:00+00:00</updated>
    <id>http://bobtfish.github.io/blog/2013/11/06/we-have-always-been-at-war-with-eastasia</id>
    <content type="html"><![CDATA[<p>Rewriting history when doing an svn to git migration is always fun. Especially when you start rewriting the entire contents of files.</p>

<p>Take this ditty for instance:</p>

<blockquote><p>   git filter-branch -f &mdash;tree-filter &lsquo;for j in $(for i in $(find . -name *.pp | grep -v vendor | grep -v tmp); do cat $i |perl -ne&#8221;/(\s+)\S/&amp;&amp;\$e{length(\$1)}++;END {exit 0 if \$e{2}; exit 2}&ldquo;|| echo $i; done); do cat $j | perl -ne&rdquo;/^(\s+)/;\$s=length(\$1)/2;\$s=\&ldquo; &#34;x\$s;s/^\s+/\$s/;print&rdquo;>&ldquo;$j.new&rdquo; &amp;&amp; mv &ldquo;$j.new&rdquo; $j; done;find . -name *.pp | grep -v vendor | grep -v tmp | xargs puppet-lint &mdash;fix >/dev/null ||true&rsquo; HEAD</p></blockquote>

<p>This switches my puppet codebase to the canonical 2 space (rather than 4 space) tabs and fixes lint/quoting issues &ndash; all the way back through history.</p>

<p>I, for one, welcome our newold code layout overlords.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Read only bind mounts and docker for unix domain sockets]]></title>
    <link href="http://bobtfish.github.io/blog/2013/10/06/read-only-bind-mounts-and-docker/"/>
    <updated>2013-10-06T23:48:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2013/10/06/read-only-bind-mounts-and-docker</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been playing around with <a href="http://docker.io">Docker</a> a load recently, and I&rsquo;ve developed a nice
little pattern for sharing sockets that I&rsquo;m gonna pass on. (By sockets, I specifically
mean <a href="http://en.wikipedia.org/wiki/Unix_domain_socket">Unix domain sockets</a>) for the purposes
of this post.</p>

<p>A lot of applications can either use a TCP socket, or a Unix domain socket to communicate &ndash; for
example <a href="http://www.fastcgi.com/drupal/">FastCGI</a> and <a href="MySQL">http://www.mysql.com/</a> both
allow you to use either mode (and in the case of mysql, both).</p>

<p>I have a small home server, and a want to run a bunch of applications that inside containers
(for security and management reasons), and these applications need to speak to a mysql server
(via a unix domain socket &ndash; which just appears to be a file on the filesystem.</p>

<p>I also want to run the mysql server inside a container &ndash; so the mechanics of getting a socket
shared between them are a little non-trivial.</p>

<p>Lets go through a worked example of how I&rsquo;ve solved this for the dovecot imap and pop3 server
talking to mysql. This is an especially fun example, as dovecot runs as root (within it&rsquo;s container)
so if someone hacked into the server through an exploit in dovecot &ndash; they can rm -rf anything
I share with that container&hellip;</p>

<p>First off, I&rsquo;ve created some <a href="http://en.wikipedia.org/wiki/Logical_Volume_Manager_%28Linux%29">LVM</a> volumes:</p>

<blockquote><p> mysql        vg0  -wi-ao   4.00g</p></blockquote>

<p>This is the mysql data directory, which will be mounted in the mysql container as /var/lib/mysql</p>

<blockquote><p> mysql_socket vg0  -wi-ao   4.00m</p></blockquote>

<p>This is going to get mounted at /socket/mysql inside containers, and will hold the mysql Unix domain socket</p>

<blockquote><p> vmail        vg0  -wi-ao  20.00g</p></blockquote>

<p>And this is my mail spool for dovecot with all the emails in it.</p>

<p>The immediate problem with this is that if the mysql socket volume is shared between multiple other containers
(as dovecot won&rsquo;t be the only app using this mysql instance), as dovecot runs as &lsquo;root&rsquo;, then if it gets hacked,
the hacker can delete the mysql socket, and any other programs trying to connect to mysql through that
socket will be affected.</p>

<p>We can&rsquo;t allow that &ndash; that would defeat the entire point of using containers for application isolation!</p>

<p>The trick is that you only need read access to use a unix domain socket that some other program has created.</p>

<p>Therefore, we can use <a href="http://docs.1h.com/Bind_mounts">bind mounts</a> to fix this &ndash; by re-binding a readonly
(-o ro) copy of the file system, and giving <em>that</em> to dovecot would stop these issues, easy&hellip;</p>

<p>So, our partitions get mounted like this:</p>

<blockquote><p>/dev/mapper/vg0-vmail on /mnt/volumes/vmail type ext3 (rw,noatime)
/dev/mapper/vg0-mysql on /mnt/volumes/mysql type ext3 (rw,noatime)
/dev/mapper/vg0-mysql_socket on /mnt/volumes/mysql_socket type ext3 (rw,noatime)
/mnt/volumes/mysql_socket on /mnt/volumes_ro/mysql_socket type none (ro,bind)</p></blockquote>

<p>The last line involves a little trick. When I tried this, to my dismay, it didn&rsquo;t work.</p>

<p>The /etc/fstab entry associated with it is:</p>

<blockquote><p>/mnt/volumes/mysql_socket /mnt/volumes_ro/mysql_socket    none    bind,ro 00 00</p></blockquote>

<p>But <a href="https://www.google.co.uk/search?q=bind+mount+read+only">as you can see</a> from the Googles,
read only bind mounts are a bit tricky &ndash; you have to re-mount them a second time to make them
read-only.</p>

<p>The trick I use here is twofold &ndash; one, I only create them with puppet (mostly using the excelent
<a href="https://forge.puppetlabs.com/AlexCline/mounts">mounts module</a> (which I only had to patch <a href="https://github.com/bobtfish/puppet-mounts/commit/abe73f82064f01bccd45289eb4f4ce51353ca364">a little bit</a>),
the associated code looks like this:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>  mounts <span class="o">{</span> <span class="s2">&quot;Mount volume ${name} bind to ro&quot;</span>:
</span><span class='line'>    <span class="nv">ensure</span> <span class="o">=</span>&gt; present,
</span><span class='line'>    <span class="nb">source</span> <span class="o">=</span>&gt; <span class="s2">&quot;/mnt/volumes/${name}&quot;</span>,
</span><span class='line'>    <span class="nv">dest</span>   <span class="o">=</span>&gt; <span class="s2">&quot;/mnt/volumes_ro/${name}&quot;</span>,
</span><span class='line'>    <span class="nb">type</span>   <span class="o">=</span>&gt; <span class="s1">&#39;none&#39;</span>,
</span><span class='line'>    <span class="nv">opts</span>   <span class="o">=</span>&gt; <span class="s1">&#39;bind,ro&#39;</span>, <span class="c"># Note bind mount ignores ro till you remount, thus the trickery below</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  -&gt;
</span><span class='line'>  <span class="nb">exec</span> <span class="o">{</span> <span class="s2">&quot;/bin/mount -o remount,ro &#39;/mnt/volumes_ro/${name}&#39;&quot;</span>:
</span><span class='line'>    <span class="nv">onlyif</span> <span class="o">=</span>&gt; <span class="s2">&quot;/bin/mount -l | /bin/grep &#39;/mnt/volumes/${name} on /mnt/volumes_ro/${name} type none (rw,bind)&#39;&quot;</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This could be notify rather than just ordering &ndash; but I like to be paranoiad and check these are ok
every puppet run..</p>

<p>However, if the machine is freshly rebooted, then the read-only file systems won&rsquo;t be remounted yet, ergo
the second piece of cunning is an upstart script to make sure things as kosher after a reboot:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="c"># /etc/init/remount-ro-bind-mounts.conf - Dirty hack to remount ro bind mounts properly</span>
</span><span class='line'>
</span><span class='line'>description <span class="s2">&quot;Remount RO bind mount filesystems on boot&quot;</span>
</span><span class='line'>
</span><span class='line'>start on <span class="nb">local</span>-filesystems
</span><span class='line'>
</span><span class='line'>console output
</span><span class='line'>
</span><span class='line'>script
</span><span class='line'>  <span class="k">for </span>fs in <span class="k">$(</span>cat /etc/fstab | awk <span class="s1">&#39;{ print $2 &quot; &quot; $4 }&#39;</span> | grep <span class="s1">&#39;bind,ro$&#39;</span> | cut -d<span class="s1">&#39; &#39;</span> -f1<span class="k">)</span>; <span class="k">do </span>mount | grep rw,bind | awk <span class="s1">&#39;{ print $3 }&#39;</span> | grep <span class="nv">$fs</span> &gt;/dev/null; <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -eq 0 <span class="o">]</span>; <span class="k">then </span>mount -o remount,ro <span class="nv">$fs</span>; <span class="k">fi</span>; <span class="k">done</span>
</span><span class='line'>end script
</span></code></pre></td></tr></table></div></figure>


<p>And there we go &ndash; all done(-ish). I just rsync the vmail spool and mysql data from another machine, and start it all up!</p>

<p>The containers look like this:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo docker ps
</span><span class='line'>ID                  IMAGE                   COMMAND             CREATED             STATUS              PORTS
</span><span class='line'>04a9da050ba8        t0m/dovecot:latest      /start run          25 minutes ago      Up 25 minutes       110-&gt;110, 143-&gt;143, 993-&gt;993, 995-&gt;995
</span><span class='line'>20b3a7a65966        t0m/mysql:latest        /start run          About an hour ago   Up About an hour
</span></code></pre></td></tr></table></div></figure>


<p>And they get run with the upstart script from <a href="https://forge.puppetlabs.com/garethr/docker">Garth&rsquo;s excelent docker puppet module</a>:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ps aux | grep <span class="s1">&#39;docker run&#39;</span> | grep -v grep
</span><span class='line'>root     16874  0.0  0.0 273744  4944 ?        Ssl  Oct06   0:00 docker run -u mysql -v /mnt/volumes/mysql/data:/var/lib/mysql -v /mnt/volumes/mysql_socket:/socket -m 0 t0m/mysql:latest
</span><span class='line'>root     28498  0.0  0.0 272336  4944 ?        Ssl  Oct06   0:00 docker run -v /mnt/volumes/vmail:/var/vmail -v /mnt/volumes_ro/mysql_socket/run:/socket/mysql -m 0 t0m/dovecot:latest
</span></code></pre></td></tr></table></div></figure>


<p>And last but not least, here&rsquo;s it working:</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl s_client -quiet -connect localhost:995
</span><span class='line'><span class="nv">depth</span><span class="o">=</span>0 <span class="nv">C</span> <span class="o">=</span> GB, <span class="nv">ST</span> <span class="o">=</span> Greater London, <span class="nv">L</span> <span class="o">=</span> London, <span class="nv">O</span> <span class="o">=</span> bobtfish.net, <span class="nv">CN</span> <span class="o">=</span> mail.bobtfish.net
</span><span class='line'>verify error:num<span class="o">=</span>18:self signed certificate
</span><span class='line'>verify <span class="k">return</span>:1
</span><span class='line'><span class="nv">depth</span><span class="o">=</span>0 <span class="nv">C</span> <span class="o">=</span> GB, <span class="nv">ST</span> <span class="o">=</span> Greater London, <span class="nv">L</span> <span class="o">=</span> London, <span class="nv">O</span> <span class="o">=</span> bobtfish.net, <span class="nv">CN</span> <span class="o">=</span> mail.bobtfish.net
</span><span class='line'>verify <span class="k">return</span>:1
</span><span class='line'>+OK Dovecot ready.
</span><span class='line'>USER bobtfish@bobtfish.net
</span><span class='line'>+OK
</span><span class='line'>PASS xxxxxx
</span><span class='line'>+OK Logged in.
</span><span class='line'>STAT
</span><span class='line'>+OK 488 15003687
</span></code></pre></td></tr></table></div></figure>


<p>Next up, doing the same trickery for postfix, and then I&rsquo;ll have a working mail infrastructure that&rsquo;s entirely containerised.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The starwars methodology]]></title>
    <link href="http://bobtfish.github.io/blog/2013/10/05/the-starwars-methodology/"/>
    <updated>2013-10-05T00:25:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2013/10/05/the-starwars-methodology</id>
    <content type="html"><![CDATA[<p>This seemingly isn&rsquo;t a well known term, and today I&rsquo;m irritated by people
thinking I perform &lsquo;magic&rsquo; by using grep, so here&rsquo;s a rant about what I call
the &ldquo;<em>Star Wars</em>&rdquo; &trade; methodology of problem solving:</p>

<p>To put it quite simply:</p>

<blockquote><p>Use the Source Luke</p></blockquote>

<p>Oftentimes grepping through the source code of the thing that&rsquo;s giving you
trouble (even if it&rsquo;s written in a language you don&rsquo;t speak) will turn up
gold.</p>

<p>Given a basic model of OO languages, it&rsquo;s easily possible to infer what&rsquo;s
going on (or what may be going on) most of the time given basic observations
and a few print statements, or even a file and line number.</p>

<p>Getting a full backtrace out of <a href="https://metacpan.org/module/JJORE/App-Stacktrace-0.09/bin/perl-stacktrace">most</a> <a href="http://isotope11.com/blog/getting-a-ruby-backtrace-from-gnu-debugger">dynamic</a> <a href="https://wiki.python.org/moin/DebuggingWithGdb">languages</a> is real easy too, even in extreme cases..</p>

<p>Even if you don&rsquo;t know for sure, some source diving will at least
help give you more concrete ideas about how the code is built, which
will allow you to frame the problem better when you do peresent it
to someone with the relevant language and/or domain knowledge.</p>

<p>Even if your assumptions were totally wrong &ndash; at at least look more
initiative (and tried to go deeper) than your average Joe, so
the expert is more likely to want to help you, as you&rsquo;re already
demonstrated a provable desire to be taught to fish, rather than
just be thrown them.</p>

<p>&lt;/rant&gt;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New blog.]]></title>
    <link href="http://bobtfish.github.io/blog/2013/07/13/new-blog/"/>
    <updated>2013-07-13T09:24:00+01:00</updated>
    <id>http://bobtfish.github.io/blog/2013/07/13/new-blog</id>
    <content type="html"><![CDATA[<p>Finally got around to creating a personal blog in which to ramble about stuff I&rsquo;m thinking about
and/or working on.</p>

<p>Expect perl, ruby, puppet, mcollective, sysadmin nonsense &amp; arduinos if you&rsquo;re lucky.</p>
]]></content>
  </entry>
  
</feed>
